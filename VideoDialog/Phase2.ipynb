{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c31b05",
   "metadata": {},
   "source": [
    "# Phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad272824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 63 captions from 10 videos:\n",
      "video_id\n",
      "AT_pPlJTiyE     9\n",
      "ELiXlJUBzzw     7\n",
      "LGS_yzsScfw     4\n",
      "VOyKKN3NdXM     6\n",
      "XwW5p80hYCg     8\n",
      "Z-6dR4H2dns     6\n",
      "dL--vW-AJJo     5\n",
      "iSH43hQoxio    10\n",
      "orwTrxIwCpo     4\n",
      "z8VqGGu5vPc     4\n",
      "dtype: int64\n",
      "\n",
      "Sample captions:\n",
      "   video_id       caption\n",
      "orwTrxIwCpo Hand car wash\n",
      "orwTrxIwCpo Hand car wash\n",
      "orwTrxIwCpo Hand car wash\n",
      "orwTrxIwCpo Hand car wash\n",
      "VOyKKN3NdXM Hand car wash\n",
      "VOyKKN3NdXM Hand car wash\n",
      "VOyKKN3NdXM Hand car wash\n",
      "VOyKKN3NdXM Hand car wash\n",
      "VOyKKN3NdXM Hand car wash\n",
      "VOyKKN3NdXM Hand car wash\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the full JSON file\n",
    "with open('captions/activity_net.v1-3.min.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Step 1: Flatten the data into a list of {video_id, caption}\n",
    "records = []\n",
    "for video_id, video_info in data['database'].items():\n",
    "    for ann in video_info['annotations']:\n",
    "        records.append({\n",
    "            'video_id': video_id,\n",
    "            'caption': ann['label']\n",
    "        })\n",
    "\n",
    "# Step 2: Convert to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Step 3: Filter for captions containing the word \"car\"\n",
    "contains_car = df[df['caption'].str.contains(r'\\bcar\\b', case=False, na=False)]\n",
    "car_video_ids = set(contains_car['video_id'])\n",
    "\n",
    "# Step 4: Filter for videos with more than 8 captions\n",
    "caption_counts = df['video_id'].value_counts()\n",
    "videos_with_8plus = set(caption_counts[caption_counts > 3].index)\n",
    "\n",
    "# Step 5: Intersection of both criteria\n",
    "valid_video_ids = list(car_video_ids & videos_with_8plus)\n",
    "\n",
    "# Step 6: Pick 10 video IDs (or fewer if not enough matches)\n",
    "selected_video_ids = valid_video_ids[:10]\n",
    "\n",
    "# Step 7: Get all captions for the selected videos\n",
    "selected_df = df[df['video_id'].isin(selected_video_ids)]\n",
    "\n",
    "# Print summary\n",
    "print(f\"Selected {len(selected_df)} captions from {len(selected_video_ids)} videos:\")\n",
    "print(selected_df.groupby(\"video_id\").size())\n",
    "\n",
    "# Optional: Preview some of the selected captions\n",
    "print(\"\\nSample captions:\")\n",
    "print(selected_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466fbead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 video IDs.\n",
      "\n",
      "Processing iSH43hQoxio from https://www.youtube.com/watch?v=iSH43hQoxio\n",
      "Failed to process iSH43hQoxio: iSH43hQoxio is unavailable\n",
      "\n",
      "Processing AT_pPlJTiyE from https://www.youtube.com/watch?v=AT_pPlJTiyE\n",
      "Failed to process AT_pPlJTiyE: HTTP Error 400: Bad Request\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from pytube import YouTube\n",
    "import av\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "JSON_FILE = \"captions/activity_net.v1-3.min.json\"\n",
    "OUTPUT_DIR = \"extracted_frames\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- STEP 1: Load video caption metadata ---\n",
    "with open(JSON_FILE, 'r') as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "records = []\n",
    "for vid, info in full_data[\"database\"].items():\n",
    "    for ann in info[\"annotations\"]:\n",
    "        records.append({\n",
    "            \"video_id\": vid,\n",
    "            \"caption\": ann[\"label\"]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# --- STEP 2: Filter videos by caption and count ---\n",
    "contains_car = df[df[\"caption\"].str.contains(r\"\\bcar\\b\", case=False, na=False)]\n",
    "car_video_ids = set(contains_car[\"video_id\"])\n",
    "\n",
    "caption_counts = df[\"video_id\"].value_counts()\n",
    "videos_with_8plus = set(caption_counts[caption_counts > 8].index)\n",
    "\n",
    "valid_video_ids = list(car_video_ids & videos_with_8plus)\n",
    "selected_video_ids = valid_video_ids[:10]\n",
    "\n",
    "print(f\"Selected {len(selected_video_ids)} video IDs.\")\n",
    "\n",
    "# --- STEP 3: Download videos and extract frames ---\n",
    "for video_id in selected_video_ids:\n",
    "    info = full_data[\"database\"][video_id]\n",
    "    url = info.get(\"url\")\n",
    "\n",
    "    print(f\"\\nProcessing {video_id} from {url}\")\n",
    "    try:\n",
    "        # Download video\n",
    "        yt = YouTube(url)\n",
    "        stream = yt.streams.filter(file_extension='mp4', progressive=True).order_by('resolution').desc().first()\n",
    "        local_path = stream.download(filename=f\"{video_id}.mp4\")\n",
    "\n",
    "        # Extract frames\n",
    "        with av.open(local_path) as container:\n",
    "            stream = container.streams.video[0]\n",
    "            stream.codec_context.skip_frame = \"NONKEY\"  # Only keyframes\n",
    "\n",
    "            for i, frame in enumerate(container.decode(stream)):\n",
    "                out_path = os.path.join(OUTPUT_DIR, f\"{video_id}_frame_{i:04d}.jpg\")\n",
    "                frame.to_image().save(out_path, quality=80)\n",
    "\n",
    "        print(f\"Frames saved for {video_id}\")\n",
    "\n",
    "        # Optional: clean up video file\n",
    "        os.remove(local_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {video_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json # Added for load_captions if it were inlined, but it's an import\n",
    "\n",
    "# --- Necessary imports from Script 1 functionality ---\n",
    "from pytube import YouTube\n",
    "import av\n",
    "import os\n",
    "\n",
    "def load_captions(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f) # data is a dict where keys are video_ids like \"v_---\"\n",
    "\n",
    "    result = []\n",
    "    # Assuming data is structured as: {\"v_VIDEOID\": {\"timestamps\": [[s,e],...], \"sentences\": [\"...\",...], \"duration\": X}}\n",
    "    for vid_key, meta_info in data.items(): # vid_key is like \"v_xxxx\"\n",
    "        # Ensure meta_info is the dictionary containing 'timestamps', 'sentences', 'duration'\n",
    "        if isinstance(meta_info, dict) and 'timestamps' in meta_info and 'sentences' in meta_info:\n",
    "            for (start, end), sentence in zip(meta_info['timestamps'], meta_info['sentences']):\n",
    "                result.append({\n",
    "                    \"video_id\": vid_key, # Store the original video key\n",
    "                    \"duration\": meta_info.get(\"duration\", 0), # Use .get for safety\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"caption\": sentence,\n",
    "                    \"video_url\": f\"https://www.youtube.com/watch?v={vid_key[2:]}\" if vid_key.startswith(\"v_\") else f\"https://www.youtube.com/watch?v={vid_key}\"\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Warning: Skipping entry {vid_key} due to unexpected structure or missing keys.\")\n",
    "\n",
    "\n",
    "    return result\n",
    "# --- End of load_captions definition ---\n",
    "\n",
    "\n",
    "# Get current working directory\n",
    "repo_root = Path().resolve()\n",
    "\n",
    "# Build the relative path to the JSON file\n",
    "# Make sure this path is correct for your environment\n",
    "json_path = repo_root / \"captions\" / \"train.json\" # Example path\n",
    "\n",
    "# --- CONFIGURATION for frame extraction (from Script 1) ---\n",
    "OUTPUT_DIR = \"extracted_frames_from_script2\" # Changed name to avoid conflict\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# --- End of CONFIGURATION ---\n",
    "\n",
    "# Load all captions\n",
    "# Ensure train.json exists at the specified path and is in the expected format for load_captions\n",
    "if not json_path.exists():\n",
    "    print(f\"Error: JSON file not found at {json_path}\")\n",
    "    print(\"Please ensure 'captions/train.json' exists relative to the script or provide the correct path.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loading captions from: {json_path}\")\n",
    "data = load_captions(str(json_path))\n",
    "if not data:\n",
    "    print(\"No data loaded from captions file. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "if df.empty:\n",
    "    print(\"DataFrame is empty after loading captions. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Step 1: Find videos that mention \"car\"\n",
    "contains_car = df[df['caption'].str.contains(r'\\bcar\\b', case=False, na=False)]\n",
    "car_video_ids = set(contains_car['video_id'])\n",
    "\n",
    "# Step 2: Get videos with more than 8 captions\n",
    "caption_counts = df['video_id'].value_counts()\n",
    "videos_with_8plus = set(caption_counts[caption_counts > 8].index)\n",
    "\n",
    "# Step 3: Intersection â†’ videos that meet both criteria\n",
    "valid_video_ids = list(car_video_ids & videos_with_8plus)\n",
    "\n",
    "# Step 4: Pick just 10 videos from the intersection\n",
    "# Renaming five_video_ids to selected_video_ids for clarity with Script 1's goal\n",
    "selected_video_ids = valid_video_ids[:10]\n",
    "\n",
    "print(f\"\\nIdentified {len(selected_video_ids)} video IDs for processing.\")\n",
    "\n",
    "# --- Get URLs for the selected video IDs ---\n",
    "# The 'data' from load_captions already has video_id and video_url.\n",
    "# We can create a mapping from video_id to its URL.\n",
    "video_id_to_url_map = {}\n",
    "if not df.empty and 'video_id' in df.columns and 'video_url' in df.columns:\n",
    "    temp_df_for_urls = df[df['video_id'].isin(selected_video_ids)][['video_id', 'video_url']].drop_duplicates()\n",
    "    video_id_to_url_map = pd.Series(temp_df_for_urls.video_url.values, index=temp_df_for_urls.video_id).to_dict()\n",
    "else:\n",
    "    print(\"DataFrame is missing 'video_id' or 'video_url' columns. Cannot proceed with URL mapping.\")\n",
    "    exit()\n",
    "\n",
    "# --- STEP 5: Download videos and extract frames (adapted from Script 1) ---\n",
    "print(f\"\\nStarting download and frame extraction for up to {len(selected_video_ids)} videos...\")\n",
    "processed_video_count = 0\n",
    "for video_id in selected_video_ids:\n",
    "    url = video_id_to_url_map.get(video_id)\n",
    "\n",
    "    if not url:\n",
    "        print(f\"Could not find URL for video_id: {video_id}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing {video_id} from {url}\")\n",
    "    local_path = \"\" # Initialize to ensure it's defined for finally block if needed\n",
    "    try:\n",
    "        # Download video\n",
    "        yt = YouTube(url)\n",
    "        # Filter for progressive MP4 streams and get the highest resolution available\n",
    "        stream = yt.streams.filter(file_extension='mp4', progressive=True).order_by('resolution').desc().first()\n",
    "\n",
    "        if not stream:\n",
    "            print(f\"No suitable MP4 stream found for {video_id} ({url}). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Downloading video: {video_id}...\")\n",
    "        # Download to a temporary path or the main script directory\n",
    "        # Using OUTPUT_DIR to keep downloads and frames somewhat together, then deleting.\n",
    "        download_filename = f\"{video_id}.mp4\"\n",
    "        local_path = stream.download(output_path=OUTPUT_DIR, filename=download_filename)\n",
    "        print(f\"Downloaded to {local_path}\")\n",
    "\n",
    "        # Extract frames\n",
    "        print(f\"Extracting frames for {video_id}...\")\n",
    "        with av.open(local_path) as container:\n",
    "            video_stream = container.streams.video[0]\n",
    "            video_stream.codec_context.skip_frame = \"NONKEY\"  # Decode only keyframes\n",
    "\n",
    "            frames_extracted_count = 0\n",
    "            for i, frame in enumerate(container.decode(video_stream)):\n",
    "                out_frame_path = os.path.join(OUTPUT_DIR, f\"{video_id}_frame_{i:04d}.jpg\")\n",
    "                frame.to_image().save(out_frame_path, quality=80)\n",
    "                frames_extracted_count += 1\n",
    "            print(f\"{frames_extracted_count} frames saved for {video_id} in {OUTPUT_DIR}\")\n",
    "        \n",
    "        processed_video_count +=1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {video_id}: {e}\")\n",
    "    finally:\n",
    "        # Optional: clean up video file\n",
    "        if local_path and os.path.exists(local_path):\n",
    "            try:\n",
    "                print(f\"Cleaning up video file: {local_path}\")\n",
    "                os.remove(local_path)\n",
    "            except Exception as e_remove:\n",
    "                print(f\"Failed to remove video file {local_path}: {e_remove}\")\n",
    "\n",
    "print(f\"\\nFinished processing. {processed_video_count} videos had frames extracted.\")\n",
    "\n",
    "# --- Original Script 2's confirmation (can be kept or removed as needed) ---\n",
    "# This part is about the selected *captions*, not the video processing.\n",
    "if not df.empty and 'video_id' in df.columns:\n",
    "    selected_captions_df = df[df['video_id'].isin(selected_video_ids)]\n",
    "    if not selected_captions_df.empty:\n",
    "        print(f\"\\nSelected {len(selected_captions_df)} captions from the processed video IDs:\")\n",
    "        print(selected_captions_df.groupby(\"video_id\").size())\n",
    "    else:\n",
    "        print(\"\\nNo captions found for the selected video IDs in the dataframe (this is unexpected if videos were processed).\")\n",
    "else:\n",
    "    print(\"\\nSkipping caption confirmation as DataFrame was not properly populated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
